{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref: \n",
    "(https://www.kaggle.com/code/ambrosm/tpsaug22-eda-which-makes-sense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from colorama import Fore, Back, Style\n",
    "import scipy.stats\n",
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, IterativeImputer, KNNImputer\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis, LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "from lightgbm import LGBMClassifier, early_stopping, log_evaluation\n",
    "\n",
    "np.set_printoptions(linewidth=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/01_raw/tabular-playground-series-aug-2022/train.csv',\n",
    "                    index_col='id')\n",
    "test = pd.read_csv('../data/01_raw/tabular-playground-series-aug-2022/test.csv',\n",
    "                    index_col='id')\n",
    "display(train)\n",
    "display(test)\n",
    "both = pd.concat([train[test.columns], test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of the 26,570 from Train set, only 21 percent failed. \n",
    "\n",
    "<b>Insights</b>: This looks slightly imbalanced. Might be good to stratify train-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.failure.value_counts() / len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing value \n",
    "mis_train = train.isna().sum().to_frame()#.reset_index()\n",
    "mis_test = test.isna().sum().to_frame()\n",
    "mis_train.columns, mis_test.columns = ['train_cols'], ['test_cols']\n",
    "missing_cols = pd.concat([mis_train, mis_test], axis=1)\n",
    "missing_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Float columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# float columns.\n",
    "float_cols = [f for f in train.columns if train[f].dtype == float]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axs = plt.subplots(4, 4, figsize=(12,12))\n",
    "for f, ax in zip(float_cols, axs.ravel()):\n",
    "    mi = min(train[f].min(), test[f].min())\n",
    "    ma = max(train[f].max(), test[f].max())\n",
    "    bins = np.linspace(mi, ma, 50)\n",
    "    ax.hist(train[f], bins=bins, alpha=0.5, density=True, label='train')\n",
    "    ax.hist(test[f], bins=bins, alpha=0.5, density=True, label='test')\n",
    "    ax.set_xlabel(f)\n",
    "    if ax == axs[0, 0]: ax.legend(loc='lower left')\n",
    "        \n",
    "    ax2 = ax.twinx()\n",
    "    total, _ = np.histogram(train[f], bins=bins)\n",
    "    failures, _ = np.histogram(train[f][train.failure == 1], bins=bins)\n",
    "    with warnings.catch_warnings(): # ignore divide by zero for empty bins\n",
    "        warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "        ax2.scatter((bins[1:] + bins[:-1]) / 2, failures / total,\n",
    "                    color='m', s=10, label='failure probability')\n",
    "    ax2.set_ylim(0, 0.5)\n",
    "    ax2.tick_params(axis='y', colors='m')\n",
    "    if ax == axs[0, 0]: ax2.legend(loc='upper right')\n",
    "plt.tight_layout(w_pad=1)\n",
    "plt.suptitle('Train and test distributions of the continuous features', fontsize=20, y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insight: With that many missing values, good value imputation will be the key for winning the competition.\n",
    "\n",
    "Let's look at the distribution of the float features. We plot the train and test histograms in the same diagram - train is blue, test is orange.\n",
    "\n",
    "We see that the first feature, loading, has a skewed distribution, perhaps log-normal; the other features are normally distributed. The first eight features have the same distribution in train and test; from measurement_10 onwards the distributions differ slightly.\n",
    "\n",
    "In the same diagrams, we show the failure probabilities with magenta dots. The top left diagram shows that higher loadings imply a higher failure probability. The bottom right diagram shows that measurement_17 is positively correlated to the target as well. All other float features seem to be uncorrelated to the failure probability (the failure probability is 21 % independent of the feature value)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binomial distribution -> test for failure count.\n",
    "\n",
    "Maybe the cause of a product failure triggers the failure of a measurement device. How can we test this idea? We calculate the conditional product failure rate given the measurement is missing E[product fails | measurement is missing] and compare it to the unconditional product failure rate, which is 0.212608."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by plotting the bell curve\n",
    "plt.figure(figsize=(12, 4))\n",
    "z_ticks = np.linspace(-3.5, 3.5, 61)\n",
    "pdf = scipy.stats.norm.pdf(z_ticks)\n",
    "plt.plot(z_ticks, pdf)\n",
    "# Calculate the conditional failure rate for every missing feature\n",
    "# Print the values and plot them\n",
    "print('feature           fail   miss   failure rate       z    p-value')\n",
    "for f in train.columns:\n",
    "    if train[f].isna().sum() > 0:\n",
    "        total = train[f].isna().sum()\n",
    "        fail = train[train[f].isna()].failure.sum()\n",
    "        z = (fail / total - 0.212608) / (np.sqrt(0.212608 * (1-0.212608)) / np.sqrt(total))\n",
    "        plt.scatter([z], [scipy.stats.norm.pdf(z)], c='r' if abs(z) > 2 else 'g', s=100)\n",
    "        print(f\"{f:15} : {fail:4} / {total:4} = {fail/total:.3f}          {z:5.2f}      {2*scipy.stats.norm.cdf(-abs(z)):.3f}\")\n",
    "        if abs(z) > 1: plt.annotate(f\"{f}: {fail / total:.3f}\",\n",
    "                                    (z, scipy.stats.norm.pdf(z)),\n",
    "                                    xytext=(0,10), \n",
    "                                    textcoords='offset points', ha='left' if z > 0 else 'right',\n",
    "                                    color='r' if abs(z) > 2 else 'g')\n",
    "            \n",
    "# Annotage the center (z=0)\n",
    "plt.vlines([0], 0, 0.05, color='g')\n",
    "plt.annotate(f\"z_score = 0\\naverage failure rate: {0.212608:.3f}\",\n",
    "                                    (0, 0.05),\n",
    "                                    xytext=(0,10), \n",
    "                                    textcoords='offset points', ha='center',\n",
    "                                    color='g')\n",
    "plt.title('Failure rate when feature is missing')\n",
    "plt.yticks([])\n",
    "plt.xlabel('z_score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When measurement_3 is missing, the failure rate is 0.160 (much lower than average).\n",
    "When measurement_5 is missing, the failure rate is 0.254 (much higher than average)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integer columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_cols = [f for f in train.columns if train[f].dtype == 'int64' and f != 'failure']\n",
    "pd.concat([train[int_cols].isna().sum().rename('missing_values in train'),\n",
    "           test[int_cols].isna().sum().rename('missing values in test')], \n",
    "          axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the distributions of the five integer features for train and test:\n",
    "_, axs = plt.subplots(2, 3, figsize=(12, 8))\n",
    "for f, ax in zip(int_cols, axs.ravel()):\n",
    "    temp1 = train.failure.groupby(train[f]).agg(['mean', 'size'])\n",
    "    ax.bar(temp1.index, temp1['size'] / len(train), alpha=0.5, label='train')\n",
    "    temp2 = test[f].value_counts()\n",
    "    ax.bar(temp2.index, temp2 / len(test), alpha=0.5, label='test')\n",
    "    ax.set_xlabel(f)\n",
    "    ax.set_ylabel('frequency')\n",
    "    \n",
    "    ax2 = ax.twinx()\n",
    "    ax2.scatter(temp1.index, temp1['mean'],\n",
    "                color='m', label='failure probability')\n",
    "    ax2.set_ylim(0, 0.5)\n",
    "    ax2.tick_params(axis='y', colors='m')\n",
    "    if ax == axs[0, 0]: ax2.legend(loc='upper right')\n",
    "\n",
    "axs[0, 0].legend()\n",
    "axs[1, 2].axis('off')\n",
    "plt.tight_layout(w_pad=1)\n",
    "plt.suptitle('Train and test distributions of the integer features', fontsize=20, y=1.02)\n",
    "plt.show()\n",
    "del temp1, temp2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see for attribute 2 values occur only in train set, 5 and 8, while attribute 7 only exists in test set. \n",
    "attribute 3 is also similar.\n",
    "Measurement_2 has positive correlation to failure (target) for values >10. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The string columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_cols = [f for f in train.columns if train[f].dtype == object]\n",
    "pd.concat([train[string_cols].isna().sum().rename('missing values in train'),\n",
    "           test[string_cols].isna().sum().rename('missing values in test')],\n",
    "          axis=1) # no missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
    "for f, ax in zip(string_cols, axs.ravel()):\n",
    "    temp1 = train[f].value_counts(dropna=False, normalize=True)\n",
    "    temp2 = test[f].value_counts(dropna=False, normalize=True)\n",
    "    values = sorted(set(temp1.index).union(temp2.index))\n",
    "    temp1 = temp1.reindex(values)\n",
    "    temp2 = temp2.reindex(values)\n",
    "    ax.bar(range(len(values)), temp1, alpha=0.5, label='train')\n",
    "    ax.bar(range(len(values)), temp2, alpha=0.5, label='test')\n",
    "    ax.set_xlabel(f)\n",
    "    ax.set_ylabel('frequency')\n",
    "    ax.set_xticks(range(len(values)), values)\n",
    "    \n",
    "    temp1 = train.failure.groupby(train[f]).agg(['mean', 'size'])\n",
    "    temp1 = temp1.reindex(values)\n",
    "    ax2 = ax.twinx()\n",
    "    ax2.scatter(range(len(values)), temp1['mean'],\n",
    "                color='m', label='failure probability')\n",
    "    ax2.tick_params(axis='y', colors='m')\n",
    "    ax2.set_ylim(0, 0.5)\n",
    "    if ax == axs[0]: ax2.legend(loc='lower right')\n",
    "\n",
    "axs[0].legend()\n",
    "plt.suptitle('Train and test distributions of the string features', fontsize=20, y=0.96)\n",
    "plt.tight_layout(w_pad=1)\n",
    "plt.show()\n",
    "del temp1, temp2  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Product codes do not overlap between train and test set. To simulate failure, we have to simulate this situation by splitting the data so that the validation set contains other products than the training set. \n",
    "\n",
    "Attribute_0 and attribute_1 are categorical features which should be one-hot encoded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import missingno as msno\n",
    "Nrows,Ncols = train.shape\n",
    "\n",
    "msno.matrix(train.iloc[np.random.choice(range(Nrows), 250)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The missingness seems similar between both the training and the test set. The product_code and the `attribute_i` features are all completely non-missing. `measurement_i`, for  i≤2  are all non-missing. `measurement_i` for  `i≥3`  have some missing values and have progressively increasing proportion of missingness. This suggests that there are a series of measurements, the results of which determine whether or not later tests are carried out. The probabilities of later measurements being missing doesn't appear to be too strongly related to the missingness of earlier measurements, as inferred from msno.heatmap (not shown here), however the dendrogram analysis from msno shows a definite pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.dendrogram(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Product codes and attributes\n",
    "both[string_cols + ['attribute_2', 'attribute_3']].drop_duplicates().set_index('product_code')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<iframe src=\"https://www.kaggle.com/embed/ambrosm/tpsaug22-eda-which-makes-sense?cellIds=28&kernelSessionId=102683452\" height=\"300\" style=\"margin: 0 auto; width: 100%; max-width: 950px;\" frameborder=\"0\" scrolling=\"auto\" title=\"TPSAUG22 EDA which makes sense ⭐️⭐️⭐️⭐️⭐️\"></iframe>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_list = []\n",
    "test_pred_list = []\n",
    "importance_list = []\n",
    "\n",
    "kf = GroupKFold(n_splits=5)\n",
    "for fold, (idx_tr,idx_va) in enumerate(kf.split(train, train.failure, train.product_code)):\n",
    "    X_tr = train.iloc[idx_tr][test.columns]\n",
    "    X_va = train.iloc[idx_va][test.columns]\n",
    "    X_te = test.copy()\n",
    "    y_tr = train.iloc[idx_tr].failure\n",
    "    y_va = train.iloc[idx_va].failure\n",
    "    \n",
    "    # one-hot encode attribute_0 and attribute_1\n",
    "    ohe_attributes = ['attribute_0', 'attribute_1']\n",
    "    ohe_output = ['ohe0_7', 'ohe1_6', 'ohe1_8']\n",
    "    ohe = OneHotEncoder(categories=[['material_5', 'material_7'],\n",
    "                                    ['material_5', 'material_6', 'material_8']],\n",
    "                        drop='first', sparse=False, handle_unknown='ignore')\n",
    "    ohe.fit(X_tr[ohe_attributes])\n",
    "    \n",
    "    for df in [X_tr, X_va, X_te]:\n",
    "        with warnings.catch_warnings(): # ignore \"Found unknown categories\"\n",
    "            warnings.filterwarnings('ignore', category=UserWarning)\n",
    "            df[ohe_output] = ohe.transform(df[ohe_attributes])\n",
    "        df.drop(columns=ohe_attributes, inplace=True)\n",
    "        \n",
    "    # We add the indicators for missing values\n",
    "    for df in [X_tr, X_va, X_te]:\n",
    "        df['m_3_missing'] = df.measurement_3.isna()\n",
    "        df['m_5_missing'] = df.measurement_5.isna()\n",
    "    \n",
    "    # We fill the missing values\n",
    "    features = [f for f in X_tr.columns if f == 'loading' or f.startswith('measurement')]\n",
    "    imputer = KNNImputer(n_neighbors=3)\n",
    "    imputer.fit(X_tr[features])\n",
    "    for df in [X_tr, X_va, X_te]:\n",
    "        df[features] = imputer.transform(df[features])\n",
    "    \n",
    "    # The EDA diagram of measurement 2 shows that the feature is correlated\n",
    "    # to the target only for values above 10. For this reason, we clip\n",
    "    # all values below 11.\n",
    "    for df in [X_tr, X_va, X_te]:\n",
    "        df['measurement_2'] = df['measurement_2'].clip(11, None)\n",
    "    \n",
    "    # We fit a model\n",
    "    features2 = [f for f in X_tr.columns if f != 'product_code']\n",
    "    model = make_pipeline(StandardScaler(), \n",
    "                          LogisticRegression(penalty='l1', C=0.01,\n",
    "                                             solver='liblinear', random_state=1))\n",
    "    model.fit(X_tr[features2], y_tr)\n",
    "    importance_list.append(model.named_steps['logisticregression'].coef_.ravel())\n",
    "    \n",
    "    # We validate the model\n",
    "    y_va_pred = model.predict_proba(X_va[features2])[:,1]\n",
    "    score = roc_auc_score(y_va, y_va_pred)\n",
    "    print(f\"Fold {fold}: auc = {score:.5f}\")\n",
    "    auc_list.append(score)\n",
    "# Show overall score\n",
    "print(f\"{Fore.GREEN}{Style.BRIGHT}Average auc = {sum(auc_list) / len(auc_list):.5f}{Style.RESET_ALL}\")\n",
    "\n",
    "# Show feature importances\n",
    "importance_df = pd.DataFrame(np.array(importance_list).T, index=features2)\n",
    "importance_df['mean'] = importance_df.mean(axis=1).abs()\n",
    "importance_df['feature'] = features2\n",
    "importance_df = importance_df.sort_values('mean', ascending=False).reset_index().head(10)\n",
    "plt.figure(figsize=(14, 4))\n",
    "plt.barh(importance_df.index, importance_df['mean'], color='lightgreen')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.yticks(ticks=importance_df.index, labels=importance_df['feature'])\n",
    "plt.title('LogisticRegression feature importances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with keras\n",
    "\n",
    "https://www.kaggle.com/code/heyspaceturtle/feature-selection-is-all-u-need?scriptVersionId=102667189"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn import preprocessing\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Modeling \n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import cluster, accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import cross_validate, GridSearchCV, cross_val_score, StratifiedKFold\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "# Other \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "train = pd.read_csv('../data/01_raw//tabular-playground-series-aug-2022/train.csv')\n",
    "test = pd.read_csv('../data/01_raw/tabular-playground-series-aug-2022/test.csv')\n",
    "sample_submission = pd.read_csv('../data/01_raw//tabular-playground-series-aug-2022/sample_submission.csv')\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable lists for easy manipulation\n",
    "id_var = ['id']\n",
    "target= ['failure']\n",
    "cat_vars = ['product_code','attribute_0','attribute_1']\n",
    "num_vars = [v for v in test.columns if v not in id_var and v not in cat_vars]\n",
    "predictors = cat_vars + num_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track missing values \n",
    "#for v in num_vars: \n",
    "#    if train[v].isna().sum() > 0: \n",
    "#        train[f'na_{v}']= np.where(train[v].isna()==True, 1,0)\n",
    "#        test[f'na_{v}']= np.where(test[v].isna()==True, 1,0)\n",
    "        \n",
    "#Imputing missing values \n",
    "multi_imp = IterativeImputer(max_iter = 9, random_state = 42, verbose = 0, skip_complete = True, n_nearest_features = 10, tol = 0.001)\n",
    "multi_imp.fit(train[num_vars])\n",
    "train[num_vars] = multi_imp.transform(train[num_vars])\n",
    "test[num_vars] = multi_imp.transform(test[num_vars])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop product code\n",
    "test = test.drop(['product_code'], axis = 1)\n",
    "train = train.drop(['product_code'], axis = 1)\n",
    "cat_vars.remove('product_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature engineering\n",
    "train['attribute_2*3'] = train['attribute_2'] * train['attribute_3']\n",
    "test['attribute_2*3'] = test['attribute_2'] * test['attribute_3']\n",
    "\n",
    "train['meas17_loading_ratio'] = train['measurement_17']/train['loading']\n",
    "test['meas17_loading_ratio'] = test['measurement_17']/test['loading']\n",
    "\n",
    "meas_cols = [f\"measurement_{i:d}\" for i in list(range(3, 17))]\n",
    "train['meas_avg'] = np.mean(train[meas_cols], axis=1)\n",
    "train['meas_std'] = np.std(train[meas_cols], axis=1)\n",
    "train['meas_max'] = np.max(train[meas_cols], axis=1)\n",
    "train['meas_min'] = np.min(train[meas_cols], axis=1)\n",
    "\n",
    "test['meas_avg'] = np.mean(test[meas_cols], axis=1)\n",
    "test['meas_std'] = np.std(test[meas_cols], axis=1) \n",
    "test['meas_max'] = np.max(test[meas_cols], axis=1)\n",
    "test['meas_min'] = np.min(test[meas_cols], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding\n",
    "label_encoder = LabelEncoder()\n",
    "train_le = train.copy()\n",
    "test_le = test.copy()\n",
    "\n",
    "for col in ['attribute_0', 'attribute_1']:\n",
    "    train_le[col] = label_encoder.fit_transform(train[col])\n",
    "    test_le[col] = label_encoder.fit_transform(test[col]) \n",
    "        \n",
    "train = train_le\n",
    "test = test_le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update predictor list\n",
    "predictors = [v for v in train.columns if v not in id_var and v not in target]\n",
    "train.shape\n",
    "\n",
    "y_class = LabelEncoder().fit_transform(train[target])\n",
    "\n",
    "# Train test split\n",
    "X_train, X_test, y_train, y_test, y_train_class, y_test_class = train_test_split(train[predictors], train[target], y_class, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection with Fischer's score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FisherScore(bt, y_train, predictors):\n",
    "    \"\"\"\n",
    "    Verbeke, W., Dejaeger, K., Martens, D., Hur, J., & Baesens, B. (2012). New insights\n",
    "    into churn prediction in the telecommunication sector: A profit driven data mining\n",
    "    approach. European Journal of Operational Research, 218(1), 211-229.\n",
    "    \"\"\"\n",
    "    # Get the unique values of dependent variable\n",
    "    target_var_val = y_train.unique()\n",
    "    \n",
    "    # Calculate FisherScore for each predictor\n",
    "    predictor_FisherScore = []\n",
    "    for v in predictors:\n",
    "        fs = np.abs(np.mean(bt.loc[y_train == target_var_val[0], v]) - np.mean(bt.loc[y_train == target_var_val[1], v])) / \\\n",
    "             np.sqrt(np.var(bt.loc[y_train == target_var_val[0], v]) + np.var(bt.loc[y_train == target_var_val[1], v]))\n",
    "        predictor_FisherScore.append(fs)\n",
    "    return predictor_FisherScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Fisher Score for all variables\n",
    "fs = FisherScore(train, train['failure'], predictors)\n",
    "fs_df = pd.DataFrame({\"predictor\":predictors, \"fisherscore\":fs})\n",
    "fs_df = fs_df.sort_values('fisherscore', ascending=False).round(3)\n",
    "fs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_cols = fs_df.head().predictor.tolist()\n",
    "fs_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise FS head.\n",
    "_, axs = plt.subplots(3, 2, figsize=(8,8))\n",
    "for f, ax in zip(fs_cols, axs.ravel()):\n",
    "    mi = min(train[f].min(), test[f].min())\n",
    "    ma = max(train[f].max(), test[f].max())\n",
    "    bins = np.linspace(mi, ma, 50)\n",
    "    ax.hist(train[f], bins=bins, alpha=0.5, density=True, label='train')\n",
    "    ax.hist(test[f], bins=bins, alpha=0.5, density=True, label='test')\n",
    "    ax.set_xlabel(f)\n",
    "    if ax == axs[0, 0]: ax.legend(loc='lower left')\n",
    "        \n",
    "    ax2 = ax.twinx()\n",
    "    total, _ = np.histogram(train[f], bins=bins)\n",
    "    failures, _ = np.histogram(train[f][train.failure == 1], bins=bins)\n",
    "    with warnings.catch_warnings(): # ignore divide by zero for empty bins\n",
    "        warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "        ax2.scatter((bins[1:] + bins[:-1]) / 2, failures / total,\n",
    "                    color='m', s=10, label='failure probability')\n",
    "    ax2.set_ylim(0, 0.5)\n",
    "    ax2.tick_params(axis='y', colors='m')\n",
    "    if ax == axs[0, 0]: ax2.legend(loc='upper right')\n",
    "plt.tight_layout(w_pad=1)\n",
    "plt.suptitle('Train and test distributions of the engineered features', fontsize=20, y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how AUC changes when adding more variables sorted by fisher score importance\n",
    "fs_scores = []\n",
    "top_n_vars = 20\n",
    "for i in range(1, top_n_vars+1):\n",
    "    if i % 10 == 0: print('Added # top vars :', i)\n",
    "    top_n_predictors = fs_df['predictor'][:i]\n",
    "    clf = LogisticRegression(max_iter = 2500)\n",
    "    fs_scores.append(cross_validate(clf, train[top_n_predictors], train['failure'],\n",
    "                                    scoring='roc_auc', cv=5, verbose=0, n_jobs=-1, return_train_score=True))\n",
    "\n",
    "# Plot\n",
    "plt.plot([s['train_score'].mean() for s in fs_scores], color='blue')\n",
    "plt.plot([s['test_score'].mean() for s in fs_scores], color='red')\n",
    "plt.xlabel('# vars')\n",
    "plt.ylabel('AUC')\n",
    "plt.legend(['train', 'test'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is observed that the number of variables does not improve AUC score of the test set after 4 vars, in fact AUC decreases as we increase the number of variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification + Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visible = Input(shape=(X_train.shape[1],))\n",
    "layer = Dense(256, activation='swish', kernel_initializer='he_normal')(visible)\n",
    "layer = Dense(128, activation='swish', kernel_initializer='he_normal')(layer)\n",
    "layer = Dense(64, activation='swish', kernel_initializer='he_normal')(layer)\n",
    "layer = Dense(32, activation='swish', kernel_initializer='he_normal')(layer)\n",
    "layer = Dense(16, activation='swish', kernel_initializer='he_normal')(layer)\n",
    "layer = Dense(8, activation='swish', kernel_initializer='he_normal')(layer)\n",
    "\n",
    "# regression + classification\n",
    "out_reg = Dense(1, activation='linear')(layer)\n",
    "out_clas = Dense(2, activation='softmax')(layer)\n",
    "\n",
    "model = Model(inputs=visible, outputs=[out_reg, out_clas])\n",
    "model.compile(loss=['mse','sparse_categorical_crossentropy'], optimizer='adam')\n",
    "plot_model(model, to_file='model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "\n",
    "## Training\n",
    "from datetime import datetime\n",
    "from tensorflow import keras\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Define the Keras TensorBoard callback.\n",
    "logdir=\"../logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "# Train the model.\n",
    "model.fit(X_train, [y_train,y_train_class], epochs=150, batch_size=32, callbacks=[callback], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "\n",
    "(https://www.kaggle.com/code/nnjjpp/adversarial-validation-detecting-data-drift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sample = pd.read_csv('../data/01_raw/tabular-playground-series-aug-2022/sample_submission.csv')\n",
    "train = pd.read_csv('../data/01_raw/tabular-playground-series-aug-2022/train.csv')\n",
    "test = pd.read_csv('../data/01_raw/tabular-playground-series-aug-2022/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These modifications to the preprocessing steps allow for variable names to \n",
    "# be preserved, which is useful/needed when plotting variable importance\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "class SimpleImputerNamed(SimpleImputer):\n",
    "    def get_feature_names_out(self):\n",
    "        return list(self.feature_names_in_)\n",
    "class OrdinalEncoderNamed(OrdinalEncoder):\n",
    "    def get_feature_names_out(self):\n",
    "        return list(self.feature_names_in_)\n",
    "class ColumnTransformerNamed(ColumnTransformer):\n",
    "    def get_feature_names_out(self):\n",
    "        names = []\n",
    "        for transformer in self.transformers_:\n",
    "            if transformer[0] == 'remainder':\n",
    "                if transformer[1] == 'passthrough':\n",
    "                    names += list(self.feature_names_in_[transformer[2]])\n",
    "                break\n",
    "            else:\n",
    "                names += transformer[1].get_feature_names_out()\n",
    "        return names\n",
    "    def fit(self, X, y=None):\n",
    "        #print('In fit method')\n",
    "        return super().fit(X,y)\n",
    "    def transform(self, X):\n",
    "        #print('In transform method')\n",
    "        transformed = super().transform(X)\n",
    "        return pd.DataFrame(transformed, columns= self.get_feature_names_out())\n",
    "    def fit_transform(self, X, y=None):\n",
    "        #print('In fit_transform method')\n",
    "        fit_transformed = super().fit_transform(X,y)\n",
    "        return pd.DataFrame(fit_transformed, columns=self.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding categorical variable attribute_0 and attribute_1\n",
    "#drop id - no prediction values\n",
    "#when setting up hyperparameter tuning, possible to include product code as grouping variable.\n",
    "#cross-method validation basis the disjoint product codes in train and test set should be GroupKFold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = train.pop('failure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = pd.concat([train.drop(['id','failure'], axis=1),\n",
    "                           test.drop(['id'], axis=1)])\n",
    "combined_data_original = combined_data.copy()\n",
    "\n",
    "missing_cols = combined_data.columns[list(combined_data.isna().sum()>0)]\n",
    "missing_cols_original = missing_cols.copy()\n",
    "\n",
    "categorical_cols = ['product_code', 'attribute_0', 'attribute_1']\n",
    "categorical_cols_original = categorical_cols.copy()\n",
    "\n",
    "preprocessing = ColumnTransformerNamed([('median_infill', SimpleImputerNamed(strategy='median'), missing_cols),\n",
    "                                        ('ordinal_encode', OrdinalEncoderNamed(), categorical_cols)],\n",
    "                                       remainder='passthrough')  \n",
    "\n",
    "# Test the preprocessing pipeline/transformer:\n",
    "preprocessing.fit_transform(combined_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_cols = list(train.columns[train.isna().sum()>0])\n",
    "categorical_cols = ['attribute_0', 'attribute_1']\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "preprocessing = ColumnTransformer([('median_infill', SimpleImputer(strategy='median'), missing_cols),\n",
    "                                   ('ordinal_encode', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value = -1), categorical_cols)],\n",
    "                                  remainder='passthrough')\n",
    "\n",
    "# Test the preprocessing pipeline/transformer:\n",
    "preprocessing.fit_transform(train.drop(['product_code'], axis=1))\n",
    "preprocessing.transform(test.drop(['product_code'], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test whether each row is a train or test. \n",
    "in_test_dataset = pd.DataFrame({'test': [0, ]*train.shape[0] + [1,]*test.shape[0]})\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "mod_xgb = Pipeline(steps = (\n",
    "    ['preprocessing', preprocessing],\n",
    "    ['xgboost', XGBRegressor(objective = 'binary:logistic',\n",
    "                             eval_metric = 'auc', \n",
    "                             random_state = 200)])\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_adversarial_model(X, y, estimator_pipeline, plot=True):\n",
    "    from sklearn.model_selection import KFold, cross_validate\n",
    "    from sklearn.metrics import make_scorer, roc_auc_score\n",
    "    cv_results = cross_validate(estimator=estimator_pipeline, \n",
    "                                X = X, y=y,\n",
    "                                scoring = make_scorer(roc_auc_score),\n",
    "                                cv = KFold(n_splits=5, random_state = 123, shuffle=True))\n",
    "    mean_cv_score = cv_results['test_score'].mean()\n",
    "    estimator_pipeline.fit(X, y)\n",
    "    print(f'Area under ROC curve (cross-validated): {mean_cv_score:.2f}')\n",
    "\n",
    "    print('\\nVariable        Importance')\n",
    "    print('--------------------------')\n",
    "    try:\n",
    "        column_heights = estimator_pipeline._final_estimator.feature_importances_\n",
    "    except AttributeError: # use absolute value of coefficients if model doesn't support feature importances\n",
    "        column_heights = np.abs(estimator_pipeline.steps[-1][1].coef_[0])\n",
    "    for x in zip(estimator_pipeline.named_steps['preprocessing'].get_feature_names_out() ,\n",
    "                 column_heights):\n",
    "        print(f'{x[0]:<15} {x[1]:.2f}')\n",
    "        \n",
    "    if plot:\n",
    "        \n",
    "        x1,y1 = zip(*filter(lambda z:z[1] > 0, \n",
    "                     zip(estimator_pipeline.named_steps['preprocessing'].get_feature_names_out() ,\n",
    "                         column_heights)))\n",
    "        sns.barplot(x=list(x1), y=np.array(y1))\n",
    "        plt.gca().tick_params(axis='x', rotation=90)\n",
    "\n",
    "fit_adversarial_model(combined_data, in_test_dataset, estimator_pipeline = mod_xgb, plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since there are no common values of product_code in the training and test data \n",
    "# (i.e. this variable is mutually exclusive), the \n",
    "# adversarial validation model discovered that we can just separate the different classes of \n",
    "# product_code to tell the training and test datasets apart. \n",
    "# Let's drop this variable and see what other variables the model can come up with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    categorical_cols.remove('product_code')\n",
    "except ValueError:\n",
    "    pass\n",
    "\n",
    "combined_data_no_product_code = combined_data.drop(['product_code'], axis=1)\n",
    "\n",
    "fit_adversarial_model(combined_data_no_product_code, in_test_dataset, estimator_pipeline = mod_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we've already saw this in EDA: `attribute_1`, `attribute_2` and `attribute_3`. Let's drop it\n",
    "try:\n",
    "    categorical_cols.remove('attribute_1')\n",
    "except ValueError:\n",
    "    pass\n",
    "combined_data2 = combined_data_no_product_code.drop(['attribute_1', 'attribute_2', 'attribute_3'], axis=1)\n",
    "fit_adversarial_model(combined_data2, in_test_dataset, estimator_pipeline = mod_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<iframe src=\"https://www.kaggle.com/embed/nnjjpp/adversarial-validation-detecting-data-drift?cellIds=23&kernelSessionId=103205636\" height=\"300\" style=\"margin: 0 auto; width: 100%; max-width: 950px;\" frameborder=\"0\" scrolling=\"auto\" title=\"Adversarial validation - detecting data drift\"></iframe>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Set up the pipeline again as we changed the input data (categorical_cols, etc.)\n",
    "combined_data = combined_data_original.copy()\n",
    "categorical_cols = categorical_cols_original.copy()\n",
    "missing_cols = missing_cols_original.copy()\n",
    "preprocessing = ColumnTransformerNamed([('median_infill', SimpleImputerNamed(strategy='median'), missing_cols),\n",
    "                                        ('ordinal_encode', OrdinalEncoderNamed(), categorical_cols)],\n",
    "                                       remainder='passthrough')  \n",
    "mod_logreg = Pipeline(steps = (['preprocessing', preprocessing],\n",
    "                               ['scaling', StandardScaler()],\n",
    "                               ['logistic_regression', LogisticRegression(penalty='l2',\n",
    "                                                                          solver='saga',\n",
    "                                                                          class_weight='balanced',\n",
    "                                                                          max_iter=2000,\n",
    "                                                                          C=10**6,\n",
    "                                                                          random_state = 200)]))\n",
    "\n",
    "fit_adversarial_model(combined_data, np.array(in_test_dataset).ravel(), estimator_pipeline = mod_logreg, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<iframe src=\"https://www.kaggle.com/embed/nnjjpp/adversarial-validation-detecting-data-drift?cellIds=26&kernelSessionId=103205636\" height=\"300\" style=\"margin: 0 auto; width: 100%; max-width: 900px;\" frameborder=\"0\" scrolling=\"auto\" title=\"Adversarial validation - detecting data drift\"></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "b5fe4e214ce3e96a1d6b6bbd9c976f9b7dd903cd15952c021c270a479d415f47"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
